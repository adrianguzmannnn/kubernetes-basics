{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __container__ is a collection of software processes under one namespace with access to an operating system kernel that it shares with other containers and little to no access between containers. A docker instance contains:\n",
    "\n",
    "1. A Docker image\n",
    "1. An execution environment\n",
    "1. A standard set of instructions\n",
    "\n",
    "While a __virtual machine__ has one or many applications, all of the necessary binaries and libraries, and the entire guest operating system, a container contains an application and its dependencies, but it shares the kernel with other containers, and is not tied to other infrastructure (other than Docker engine on the host). It runs isolated processes in user space on the host OS.\n",
    "\n",
    "With application containers, there is a need to:\n",
    "\n",
    "1. Instatiate containers on a host\n",
    "1. Restart failing containers\n",
    "1. Expose containers as services outside the cluster\n",
    "1. Scaling the cluster up or down\n",
    "\n",
    "One can use __Docker__ containers to develop and build applications, and then use __Kubernetes__ to run these applications on infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](kubernetes-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a __Kubernetes__ cluster, the __master__ is responsible for managing the cluster as it coordinates all of the activities and communicates with the nodes to keep kubernetes and the applications running. The __node__ serves as a worker machine. An application runs on a node. A __node__ must have the following:\n",
    "\n",
    "1. A kubelet running\n",
    "1. Container tooling like Docker\n",
    "1. A kube-proxy process running\n",
    "1. Supervisord\n",
    "\n",
    "For this course, we will be using Minikube, which is a lightweight Kubernetes implementation that creates a VM on your local machine and deploys a simple cluster containing only one node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pods__ are basic building blocks in Kubernetes. It is the simplest unit that you can interact with. You can create, deploy, and delete pods. A pod represents one running process in the cluster. A pod contains:\n",
    "\n",
    "1. The docker application container\n",
    "1. Storage resources\n",
    "1. A unique network IP\n",
    "1. Options that govern how the container should work\n",
    "\n",
    "__It is recommended to NOT use a pod directly and to use a controller (like a deployment) to manage it instead.__\n",
    "\n",
    "They are managed by controllers, which provide application reliability, scaling, and load-balancing. These include:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kinds of controllers:\n",
    "\n",
    "1. __ReplicaSets__: ensures that the specified number of replicas for a pod are running at all times (if one pod crashes and the overall number of pods is less than the expected, this controller will start up another pod)\n",
    "1. __Deployments__: provides declarative updates for pods and ReplicaSets (a deployment manages a ReplicaSet, which manages a pod). Depoyments can create new ReplicaSets. Most applications are packaged as deployments\n",
    "1. __DaemonSets__: ensures that all nodes run a copy of a specific pod\n",
    "1. __Jobs__: a supervisor process for pods carrying out batch jobs\n",
    "1. __Services__: allows communication between one set of deployments with another. Allows one set of pods to communicate with another set of pods in an easy way. Kinds of services: a) internal: IP is only reachable within the cluster, b) external: endpoint available through Node IP: port (NodePort), c) load balancer: exposes application to the internet with a load balancer (available with a cloud provider) \n",
    "\n",
    "For annotation and organization:\n",
    "\n",
    "1. __Labels__: key/value pairs that are attached to objects like pods, services, and deployments (used to identify attributes of an object i.e. \"release\": \"stable\", \"environment\": \"dev\") \n",
    "1. __Selectors__: Two types: equality or set. Used with labels to identify a specific set of objects. Used with `kubectl`.\n",
    "1. __Namespace__: For large enterprises -- allows teams to access resources with accountability. A way to divide cluster resources among users. The names of resources like deployments and pods must be unique in namespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!minikube start # https://minikube.sigs.k8s.io/docs/start/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get nodes # https://kubernetes.io/docs/tasks/tools/install-kubectl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get all # returns all resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running your first helloworld\n",
    "\n",
    "## Chapter Goals\n",
    "\n",
    "1. Starting up minikube\n",
    "2. Set up your first Kubernetes helloworld application\n",
    "3. Run your application in a minikube environment\n",
    "4. Expose application via a nodeport and see it running\n",
    "\n",
    "### Starting up minikube\n",
    "\n",
    "First, get minikube up and running with the command `minikube start`. This command sets up a Kubernetes dev environment for you via VirtualBox.\n",
    "\n",
    "The last statement in the output states that kubectl can talk to minikube. We can verify this by running the command `kubectl get nodes`\n",
    "\n",
    "This will show you that minikube is ready to use.\n",
    "\n",
    "### Set up your helloworld\n",
    "\n",
    "Make sure you have your files unzipped to your local machine (for example /documents/Kubernetes). You should be in your existing directory with the exercise files for chapter 03_04 as shown below.\n",
    "\n",
    "```\n",
    "$ pwd\n",
    "/Users/kgaekwad/Documents/Kubernetes/03_04\n",
    "$ ls -al\n",
    "total 16\n",
    "drwxr-xr-x   4 kgaekwad  staff   128 Apr 14 03:41 .\n",
    "drwxr-xr-x  22 kgaekwad  staff   704 Apr 14 03:25 ..\n",
    "-rw-r--r--   1 kgaekwad  staff  3035 Apr 14 03:55 Readme.md\n",
    "-rw-r--r--@  1 kgaekwad  staff   448 Apr 14 08:01 helloworld.yaml\n",
    "```\n",
    "\n",
    "We will run one of the most common Docker helloworld applications out there- [https://hub.docker.com/r/karthequian/helloworld/]\n",
    "\n",
    "To run this, type:\n",
    "\n",
    "```\n",
    "kubectl create -f helloworld.yaml\n",
    "```\n",
    "\n",
    "This command creates a deployment resource from the file helloworld.yaml, which, in this case, contains a deployment called \"hellworld\", pulling from the image karthequian/helloworld, and exposes port 80 of the container to the pod.\n",
    "\n",
    "Running this command will give you this output, stating that the deployment \"hw\" was created.\n",
    "\n",
    "```\n",
    "$ kubectl create -f helloworld.yaml \n",
    "deployment.apps/helloworld created\n",
    "```\n",
    "\n",
    "We can run the command `kubectl get all` to see all our resources running, as shown in the output below.\n",
    "\n",
    "```\n",
    "$ kubectl get all\n",
    "NAME                              READY   STATUS    RESTARTS   AGE\n",
    "pod/helloworld-7cf6df685c-kpnjv   1/1     Running   0          99s\n",
    "\n",
    "NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\n",
    "service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        53m\n",
    "\n",
    "NAME                         READY   UP-TO-DATE   AVAILABLE   AGE\n",
    "deployment.apps/helloworld   1/1     1            1           99s\n",
    "\n",
    "NAME                                    DESIRED   CURRENT   READY   AGE\n",
    "replicaset.apps/helloworld-7cf6df685c   1         1         1       99s\n",
    "$ \n",
    "\n",
    "```\n",
    "\n",
    "### Setting up a load balancer\n",
    "You'll notice that in the `kubectl get all` command, the service has a port mapping defined; however, when you try to hit that port via your web browser, you won't be able to reach the service.\n",
    "\n",
    "This is because by default, the pod is only accessible by its internal IP address within the cluster. To make the helloworld container accessible from outside the Kubernetes virtual network, you have to expose the pod as a Kubernetes service.\n",
    "\n",
    "To do this, we can expose the pod to the public internet using the kubectl expose command \n",
    "`kubectl expose deployment helloworld --type=NodePort`\n",
    "\n",
    "The `--type=NodePort` flag exposes the deployment outside of the cluster. If you're using this on a cloud provider, you can use a `--type=LoadBalancer` that will provision an external IP address would be provisioned to access the service.\n",
    "\n",
    "To view the final user interface, use the minikube service command.\n",
    "\n",
    "`minikube service helloworld`\n",
    "\n",
    "This will open your web browser to your application that is running in Kubernetes!\n",
    "\n",
    "\n",
    "#### Commands run in this section\n",
    "```\n",
    "pwd\n",
    "ls -al\n",
    "kubectl get all\n",
    "kubectl create -f helloworld.yaml\n",
    "kubectl expose deployment helloworld --type=NodePort\n",
    "minikube service helloworld\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file helloworld.yaml\n",
    "\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: helloworld\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: helloworld\n",
    "  replicas: 1 # tells deployment to run 1 pods matching the template\n",
    "  template: # create pods using pod definition in this template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: helloworld\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: helloworld\n",
    "        image: karthequian/helloworld:latest\n",
    "        ports:\n",
    "        - containerPort: 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking down the helloworld app\n",
    "\n",
    "## Chapter Goals\n",
    "1. Understand the basics of the deployment and service\n",
    "2. Understand what we created\n",
    "3. Create a simple single yaml file for deployment and service\n",
    "4. Run the yaml file in minikube\n",
    "5. Verify that the application is working as expected\n",
    "\n",
    "### Understand the basics of the deployment and service\n",
    "\n",
    "Running `kubectl get all` shows us pods, services and deployments that are running that build the helloworld application. Let's peel this onion and figure out what's going on.\n",
    "\n",
    "To take a look at the deployment YAML that runs the application, run `kubectl get deploy/hw -o yaml`. This will return the YAML that composes the helloworld service.\n",
    "\n",
    "When working with Kubernetes, you should get familiar with these YAML files.\n",
    "\n",
    "The Kubernetes service also comprises YAMLs. Let's take a look at that by running `kubectl get service helloworld-service -o yaml`.\n",
    "\n",
    "### Create our deployment using YAML\n",
    "\n",
    "If we were going to recreate our deployment and service as YAMLs, they would look like the following:\n",
    "\n",
    "Deployment:\n",
    "```\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: helloworld-deployment\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: helloworld\n",
    "  replicas: 1 # tells deployment to run 1 pods matching the template\n",
    "  template: # create pods using pod definition in this template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: helloworld\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: helloworld\n",
    "        image: karthequian/helloworld:latest\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "```\n",
    "\n",
    "Service:\n",
    "```\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: helloworld-service\n",
    "spec:\n",
    "  # if your cluster supports it, uncomment the following to automatically create\n",
    "  # an external load-balanced IP for the frontend service.\n",
    "  type: LoadBalancer\n",
    "  ports:\n",
    "  - port: 80\n",
    "    protocol: TCP\n",
    "    targetPort: 80\n",
    "  selector:\n",
    "    app: helloworld\n",
    "```\n",
    "\n",
    "\n",
    "### Execute our deployment using YAML\n",
    "\n",
    "To create these, we can run the command `kubectl create -f helloworld-deployment.yml` to create our deployment and `kubectl create -f helloworld-service.yml` to create the service. This will take the contents of the YAML file and create the necessary components in the file.\n",
    "\n",
    "```\n",
    "karthik$ kubectl create -f helloworld-deployment.yml\n",
    "deployment \"helloworld-deployment\" created\n",
    "karthik$ kubectl create -f helloworld-service.yml\n",
    "service \"helloworld-service\" created\n",
    "karthik$\n",
    "\n",
    "```\n",
    "\n",
    "Typically, in the real world, you would mostly not use seperate files to break up your application and would have it in a single file that encompasses the entire application with both the deployment and the service component. An example of this YAML file is shown here:\n",
    "\n",
    "```\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: helloworld-deployment\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: helloworld\n",
    "  replicas: 1 # tells deployment to run 1 pods matching the template\n",
    "  template: # create pods using pod definition in this template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: helloworld\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: helloworld\n",
    "        image: karthequian/helloworld:latest\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: helloworld-service\n",
    "spec:\n",
    "  # if your cluster supports it, uncomment the following to automatically create\n",
    "  # an external load-balanced IP for the frontend service.\n",
    "  type: LoadBalancer\n",
    "  ports:\n",
    "  - port: 80\n",
    "    protocol: TCP\n",
    "    targetPort: 80\n",
    "  selector:\n",
    "    app: helloworld\n",
    "```\n",
    "\n",
    "Notice the `---` that marks the end of one section and starts another.\n",
    "\n",
    "### Verify that the application is working as expected\n",
    "Finally, to see this new helloworld working as expected, we will run the minikube command to expose the service in the browser with the following command:\n",
    "\n",
    "```\n",
    "karthik$ minikube service helloworld-service\n",
    "Opening kubernetes service default/helloworld-service in default browser...\n",
    "karthik$\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file helloworld.yaml\n",
    "\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: helloworld-all-deployment\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: helloworld\n",
    "  replicas: 1 # tells deployment to run 1 pods matching the template\n",
    "  template: # create pods using pod definition in this template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: helloworld\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: helloworld\n",
    "        image: karthequian/helloworld:latest\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: helloworld-all-service\n",
    "spec:\n",
    "  type: NodePort\n",
    "  ports:\n",
    "  - port: 80\n",
    "    protocol: TCP\n",
    "    targetPort: 80\n",
    "  selector:\n",
    "    app: helloworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling the helloworld app\n",
    "\n",
    "## Chapter Goal\n",
    "1. Learn how to scale the helloworld application\n",
    "\n",
    "### Learn how to scale the helloworld application\n",
    "\n",
    "When we run the `kubectl get deployments` command, we notice that there's a single instance of the helloworld deployment running, as well as a single pod associated with the deployment.\n",
    "\n",
    "There are many scenarios in which this is undesired. If for some reason the pod crashes or ends up in a crash loop, we will not have any instances of the application running, which will cause downtime until the pod can restart successfully.\n",
    "\n",
    "Fortunately, we can use the inbuilt property of Kubernetes, called replica sets, to solve this issue for deployments. \n",
    "\n",
    "To run a replica set for our helloworld deployment, run the command `kubectl scale --replicas=3 deploy/helloworld-deployment`. This will add three replicas for our deployment, which effectively means three pods running for a single deployment.\n",
    "\n",
    "```\n",
    "$ kubectl scale --replicas=3 deploy/helloworld-deployment\n",
    "deployment \"helloworld-deployment\" scaled\n",
    "```\n",
    "\n",
    "Now, if we run `kubectl get all`, we'll see three pods instead of one.\n",
    "\n",
    "```\n",
    "$ kubectl get all\n",
    "NAME                                        READY     STATUS    RESTARTS   AGE\n",
    "po/helloworld-deployment-2148054017-6fc7f   1/1       Running   0          6h\n",
    "po/helloworld-deployment-2148054017-88nd8   1/1       Running   0          53m\n",
    "po/helloworld-deployment-2148054017-dvg47   1/1       Running   0          53m\n",
    "\n",
    "NAME                     CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE\n",
    "svc/helloworld-service   10.0.0.244   <pending>     80:32138/TCP   6h\n",
    "svc/kubernetes           10.0.0.1     <none>        443/TCP        9d\n",
    "\n",
    "NAME                           DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\n",
    "deploy/helloworld-deployment   3         3         3            3           6h\n",
    "\n",
    "NAME                                  DESIRED   CURRENT   READY     AGE\n",
    "rs/helloworld-deployment-2148054017   3         3         3         6h\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding labels to the application\n",
    "\n",
    "## Chapter Goal\n",
    "1. Adding labels during build time\n",
    "2. Viewing labels\n",
    "2. Adding labels to running pods\n",
    "3. Deleting a label\n",
    "4. Searching by labels\n",
    "5. Extending the label concept to deployments/services\n",
    "\n",
    "### Adding labels during build time\n",
    "\n",
    "You can add labels to pods, services and deployments either at build time or at run time. If you're adding labels at build time, you can add a label section in the metadata portion of the YAML as shown below:\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "You can deploy the code above by using the command `kubectl create -f helloworld-pod-with-labels.yml`\n",
    "\n",
    "\n",
    "### Viewing labels\n",
    "\n",
    "You have a pod with labels. Super! But how do you see them? You can add the `--show-labels` option to your kubectl get command as shown here: `kubectl get pods --show-labels`.\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "### Adding labels to running pods\n",
    "To add labels to a running pod, you can use the `kubectl label` command as follows: `kubectl label po/helloworld app=helloworld`. This adds the label `app` with the value `helloworld` to the pod.\n",
    "\n",
    "To update the value of a label, use the `--overwrite` flag in the command as follows: `kubectl label po/helloworld app=helloworldapp --overwrite` \n",
    "\n",
    "### Deleting a label\n",
    "To remove an existing label, just add a `-` to the end of the label key as follows: `kubectl label po/helloworld app-`. This will remove the app label from the helloworld pod.\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "### Searching by labels\n",
    "Creating, getting and deleting labels is nice, but the ability to search using labels helps us identify what's going on in our infrastructure better. Let's take a look. First, we're going to deploy a few pods that will constitute what a small org might have. \n",
    "\n",
    "`kubectl create -f kubectl create -f sample-infrastructure-with-labels.yml`\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "Looking at these applications running from a high level makes it hard to see what's going on with the infrastructure.\n",
    "\n",
    "`kubectl get pods`\n",
    "\n",
    "`kubectl get pods --show-labels`\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "You can search for labels with the flag `--selector` (or `-l`). If you want to search for all the pods that are running in production, you can run `kubectl get pods --selector env=production` as shown below:\n",
    "\n",
    "`kubectl get pods --selector env=production`\n",
    "\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "Similarly, to get all pods by dev lead Karthik, you'd add `dev-lead=karthik` to the selector as shown below.\n",
    "\n",
    "`kubectl get pods --selector dev-lead=karthik`\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "You can also do more complicated searches, like finding any pods owned by Karthik in the development tier, by the following query `dev-lead=karthik,env=staging`:\n",
    "\n",
    "`kubectl get pods -l dev-lead=karthik,env=staging`\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "Or, any apps not owned by Karthik in staging (using the ! construct):\n",
    "\n",
    "`kubectl get pods -l dev-lead!=karthik,env=staging`\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "Querying also supports the `in` keyword\n",
    "\n",
    "`kubectl get pods -l 'release-version in (1.0,2.0)'`\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "Or a more complicated example:\n",
    "\n",
    "`kubectl get pods -l \"release-version in (1.0,2.0),team in (marketing,ecommerce)\"`\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "The opposite of \"in\" is \"notin\". Surprise. \"Notin\" is supported as well, as shown in this example:\n",
    "\n",
    "`kubectl get pods -l 'release-version notin (1.0,2.0)'`\n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "Finally, sometimes your label might not have a value assigned to it, but you can still search by label name.\n",
    "\n",
    "`kubectl get pods -l 'release-version'`  \n",
    "\n",
    "```\n",
    "```\n",
    "\n",
    "### Extending the label concept to deployments/services\n",
    "\n",
    "As a bonus, labels will also work with `kubectl get services/deployments/all --show-labels` and will return labels for your services, deployments or all objects.\n",
    "\n",
    "`kubectl get all --show-labels`\n",
    "\n",
    "And, you can delete pods, services or deployments by label as well! For example `kubectl delete pods -l dev-lead=karthik` will delete all pods who's dev-lead was Karthik. \n",
    "\n",
    "\n",
    "To summarize, labels in Kubernetes is a powerful concept! Use the labeling feature to your advantage to build your infrastructure in an organized fashion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file helloworld-pod-with-labels.yml\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: helloworld\n",
    "  labels:\n",
    "    env: production\n",
    "    author: karthequian\n",
    "    application_type: ui\n",
    "    release-version: \"1.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: helloworld\n",
    "    image: karthequian/helloworld:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create -f helloworld-pod-with-labels.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods --show-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl label pod/helloworld app=helloworldapp --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working with sample infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file sample-infrastructure-with-labels.yml\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: homepage-dev\n",
    "  labels:\n",
    "    env: development\n",
    "    dev-lead: karthik\n",
    "    team: web\n",
    "    application_type: ui\n",
    "    release-version: \"12.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: helloworld\n",
    "    image: karthequian/helloworld:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: homepage-staging\n",
    "  labels:\n",
    "    env: staging\n",
    "    team: web\n",
    "    dev-lead: karthik\n",
    "    application_type: ui\n",
    "    release-version: \"12.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: helloworld\n",
    "    image: karthequian/helloworld:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: homepage-prod\n",
    "  labels:\n",
    "    env: production\n",
    "    team: web\n",
    "    dev-lead: karthik\n",
    "    application_type: ui\n",
    "    release-version: \"12.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: helloworld\n",
    "    image: karthequian/helloworld:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: login-dev\n",
    "  labels:\n",
    "    env: development\n",
    "    team: auth\n",
    "    dev-lead: jim\n",
    "    application_type: api\n",
    "    release-version: \"1.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: login\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: login-staging\n",
    "  labels:\n",
    "    env: staging\n",
    "    team: auth\n",
    "    dev-lead: jim\n",
    "    application_type: api\n",
    "    release-version: \"1.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: login\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: login-prod\n",
    "  labels:\n",
    "    env: production\n",
    "    team: auth\n",
    "    dev-lead: jim\n",
    "    application_type: api\n",
    "    release-version: \"1.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: login\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: cart-dev\n",
    "  labels:\n",
    "    env: development\n",
    "    team: ecommerce\n",
    "    dev-lead: carisa\n",
    "    application_type: api\n",
    "    release-version: \"1.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: cart\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: cart-staging\n",
    "  labels:\n",
    "    env: staging\n",
    "    team: ecommerce\n",
    "    dev-lead: carisa\n",
    "    application_type: api\n",
    "    release-version: \"1.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: cart\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: cart-prod\n",
    "  labels:\n",
    "    env: production\n",
    "    team: ecommerce\n",
    "    dev-lead: carisa\n",
    "    application_type: api\n",
    "    release-version: \"1.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: cart\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: social-dev\n",
    "  labels:\n",
    "    env: development\n",
    "    team: marketing\n",
    "    dev-lead: carisa\n",
    "    application_type: api\n",
    "    release-version: \"2.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: social\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: social-staging\n",
    "  labels:\n",
    "    env: staging\n",
    "    team: marketing\n",
    "    dev-lead: marketing\n",
    "    application_type: api\n",
    "    release-version: \"1.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: social\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: social-prod\n",
    "  labels:\n",
    "    env: production\n",
    "    team: marketing\n",
    "    dev-lead: marketing\n",
    "    application_type: api\n",
    "    release-version: \"1.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: social\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: catalog-dev\n",
    "  labels:\n",
    "    env: development\n",
    "    team: ecommerce\n",
    "    dev-lead: daniel\n",
    "    application_type: api\n",
    "    release-version: \"4.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: catalog\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: catalog-staging\n",
    "  labels:\n",
    "    env: staging\n",
    "    team: ecommerce\n",
    "    dev-lead: daniel\n",
    "    application_type: api\n",
    "    release-version: \"4.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: catalog\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: catalog-prod\n",
    "  labels:\n",
    "    env: production\n",
    "    team: ecommerce\n",
    "    dev-lead: daniel\n",
    "    application_type: api\n",
    "    release-version: \"4.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: catalog\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: quote-dev\n",
    "  labels:\n",
    "    env: development\n",
    "    team: ecommerce\n",
    "    dev-lead: amy\n",
    "    application_type: api\n",
    "    release-version: \"2.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: quote\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: quote-staging\n",
    "  labels:\n",
    "    env: staging\n",
    "    team: ecommerce\n",
    "    dev-lead: amy\n",
    "    application_type: api\n",
    "    release-version: \"2.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: quote\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: quote-prod\n",
    "  labels:\n",
    "    env: production\n",
    "    team: ecommerce\n",
    "    dev-lead: amy\n",
    "    application_type: api\n",
    "    release-version: \"1.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: quote\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: ordering-dev\n",
    "  labels:\n",
    "    env: development\n",
    "    team: purchasing\n",
    "    dev-lead: chen\n",
    "    application_type: backend\n",
    "    release-version: \"2.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: ordering\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: ordering-staging\n",
    "  labels:\n",
    "    env: staging\n",
    "    team: purchasing\n",
    "    dev-lead: chen\n",
    "    application_type: backend\n",
    "    release-version: \"2.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: ordering\n",
    "    image: karthequian/ruby:latest\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: ordering-prod\n",
    "  labels:\n",
    "    env: production\n",
    "    team: purchasing\n",
    "    dev-lead: chen\n",
    "    application_type: backend\n",
    "    release-version: \"2.0\"\n",
    "spec:\n",
    "  containers:\n",
    "  - name: ordering\n",
    "    image: karthequian/ruby:latest\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create -f sample-infrastructure-with-labels.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods --selector env=production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -l 'release-version in (1.0,2.0)' --show-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pods -l 'release-version notin (1.0,2.0)' --show-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete pods -l dev-lead=karthik # deletes pods based on the filter applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application health checks\n",
    "\n",
    "## Chapter Goal\n",
    "1. Add HTTP health checks to the helloworld deployment\n",
    "2. Simulate a failing deployment that fails a readiness probe\n",
    "3. Simulate a failing deployment that fails a liveness probe\n",
    "\n",
    "### Add HTTP health check to the helloworld deployment\n",
    "\n",
    "We will take our existing hellworld deployment, and add a readiness and liveness probe healthchecks.\n",
    "\n",
    "```\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: helloworld-deployment\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: helloworld\n",
    "  replicas: 1 # tells deployment to run 1 pods matching the template\n",
    "  template: # create pods using pod definition in this template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: helloworld\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: helloworld\n",
    "        image: karthequian/helloworld:latest\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "```\n",
    "\n",
    "A readiness probe is used to know when a container is ready to start accepting traffic.\n",
    "\n",
    "The yaml has the following parameters:\n",
    "```\n",
    "readinessProbe:\n",
    "  # length of time to wait for a pod to initialize\n",
    "  # after pod startup, before applying health checking\n",
    "  initialDelaySeconds: 10\n",
    "  # Amount of time to wait before timing out\n",
    "  initialDelaySeconds: 1\n",
    "  # Probe for http\n",
    "  httpGet:\n",
    "    # Path to probe\n",
    "    path: /\n",
    "    # Port to probe\n",
    "    port: 80\n",
    "```\n",
    "\n",
    "A liveness probe is used to know when a container might need to be restarted.\n",
    "\n",
    "A liveness probe yaml has the following parameters that need to be filled out:\n",
    "\n",
    "```\n",
    "livenessProbe:\n",
    "  # length of time to wait for a pod to initialize\n",
    "  # after pod startup, before applying health checking\n",
    "  initialDelaySeconds: 10\n",
    "  # Amount of time to wait before timing out\n",
    "  timeoutSeconds: 1\n",
    "  # Probe for http\n",
    "  httpGet:\n",
    "    # Path to probe\n",
    "    path: /\n",
    "    # Port to probe\n",
    "    port: 80\n",
    "```\n",
    "\n",
    "Thus, our deployment yaml now becomes:\n",
    "\n",
    "```\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: helloworld-deployment-with-probe\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: helloworld\n",
    "  replicas: 1 # tells deployment to run 1 pods matching the template\n",
    "  template: # create pods using pod definition in this template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: helloworld\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: helloworld\n",
    "        image: karthequian/helloworld:latest\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "        readinessProbe:\n",
    "          # length of time to wait for a pod to initialize\n",
    "          # after pod startup, before applying health checking\n",
    "          initialDelaySeconds: 10\n",
    "          # Amount of time to wait before timing out\n",
    "          initialDelaySeconds: 1\n",
    "          # Probe for http\n",
    "          httpGet:\n",
    "            # Path to probe\n",
    "            path: /\n",
    "            # Port to probe\n",
    "            port: 80\n",
    "        livenessProbe:\n",
    "          # length of time to wait for a pod to initialize\n",
    "          # after pod startup, before applying health checking\n",
    "          initialDelaySeconds: 10\n",
    "          # Amount of time to wait before timing out\n",
    "          timeoutSeconds: 1\n",
    "          # Probe for http\n",
    "          httpGet:\n",
    "            # Path to probe\n",
    "            path: /\n",
    "            # Port to probe\n",
    "            port: 80\n",
    "\n",
    "```\n",
    "\n",
    "We run this yaml the same as we had done before: `kubectl create -f helloworld-deployment-with-probes`\n",
    "\n",
    "\n",
    "### Simulate a failing deployment that fails a readiness probe\n",
    "We will now try to simulate a bad helloworld pod that fails a readiness probe. Instead of checking port 80 like the last example, we will run a readiness check on port 90 to simulate a failing scenario. Thus, our yaml now is:\n",
    "\n",
    "```\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: helloworld-deployment-with-bad-readiness-probe\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: helloworld\n",
    "  replicas: 1 # tells deployment to run 1 pods matching the template\n",
    "  template: # create pods using pod definition in this template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: helloworld\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: helloworld\n",
    "        image: karthequian/helloworld:latest\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "        readinessProbe:\n",
    "          # length of time to wait for a pod to initialize\n",
    "          # after pod startup, before applying health checking\n",
    "          initialDelaySeconds: 10\n",
    "          # Amount of time to wait before timing out\n",
    "          initialDelaySeconds: 1\n",
    "          # Probe for http\n",
    "          httpGet:\n",
    "            # Path to probe\n",
    "            path: /\n",
    "            # Port to probe\n",
    "            port: 90\n",
    "```\n",
    "\n",
    "We will run this yaml with the command `kubectl create -f helloworld-with-bad-readiness-probe.yaml`. After about a minute, we will notice that our pod is still not in a read state when we run the `kubectl get pods` command.\n",
    "\n",
    "```\n",
    "MacbookHome:04_02 Application health checks karthik$ kubectl get pods\n",
    "NAME                                                              READY     STATUS    RESTARTS   AGE\n",
    "helloworld-deployment-with-bad-readiness-probe-8664db7448-pv4fm   0/1       Running   0          1m\n",
    "```\n",
    "\n",
    "Describing the pod will show that the pod failed readiness:\n",
    "```\n",
    "MacbookHome:04_02 Application health checks karthik$ kubectl describe po helloworld-deployment-with-bad-readiness-probe-8664db7448-pv4fm\n",
    "Name:   helloworld-deployment-with-bad-readiness-probe-8664db7448-pv4fm\n",
    "Namespace:  default\n",
    "Node:   minikube/192.168.99.101\n",
    "Start Time: Sun, 29 Oct 2017 01:35:08 -0500\n",
    "Labels:   app=helloworld\n",
    "    pod-template-hash=4220863004\n",
    "Annotations:  kubernetes.io/created-by={\"kind\":\"SerializedReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"ReplicaSet\",\"namespace\":\"default\",\"name\":\"helloworld-deployment-with-bad-readiness-probe-8664db7448\",\"uid\"...\n",
    "Status:   Running\n",
    "IP:   172.17.0.4\n",
    "Controllers:  ReplicaSet/helloworld-deployment-with-bad-readiness-probe-8664db7448\n",
    "Containers:\n",
    "  helloworld:\n",
    "    Container ID: docker://61fa8b27fe2e8239d5dc84fc097af17c9d4decc90b3bea09103cc9d73302f235\n",
    "    Image:    karthequian/helloworld:latest\n",
    "    Image ID:   docker-pullable://karthequian/helloworld@sha256:165f87263f1775f0bf91022b48a51265357ba9f36bc3882f5ecdefc7f8ef8f6d\n",
    "    Port:   80/TCP\n",
    "    State:    Running\n",
    "      Started:    Sun, 29 Oct 2017 01:35:10 -0500\n",
    "    Ready:    False\n",
    "    Restart Count:  0\n",
    "    Readiness:    http-get http://:90/ delay=1s timeout=1s period=10s #success=1 #failure=3\n",
    "    Environment:  <none>\n",
    "    Mounts:\n",
    "      /var/run/secrets/kubernetes.io/serviceaccount from default-token-x8qf6 (ro)\n",
    "Conditions:\n",
    "  Type    Status\n",
    "  Initialized   True\n",
    "  Ready   False\n",
    "  PodScheduled  True\n",
    "Volumes:\n",
    "  default-token-x8qf6:\n",
    "    Type: Secret (a volume populated by a Secret)\n",
    "    SecretName: default-token-x8qf6\n",
    "    Optional: false\n",
    "QoS Class:  BestEffort\n",
    "Node-Selectors: <none>\n",
    "Tolerations:  <none>\n",
    "Events:\n",
    "  FirstSeen LastSeen  Count From      SubObjectPath     Type    Reason      Message\n",
    "  --------- --------  ----- ----      -------------     --------  ------      -------\n",
    "  5m    5m    1 default-scheduler         Normal    Scheduled   Successfully assigned helloworld-deployment-with-bad-readiness-probe-8664db7448-pv4fm to minikube\n",
    "  5m    5m    1 kubelet, minikube         Normal    SuccessfulMountVolume MountVolume.SetUp succeeded for volume \"default-token-x8qf6\"\n",
    "  5m    5m    1 kubelet, minikube spec.containers{helloworld} Normal    Pulling     pulling image \"karthequian/helloworld:latest\"\n",
    "  5m    5m    1 kubelet, minikube spec.containers{helloworld} Normal    Pulled      Successfully pulled image \"karthequian/helloworld:latest\"\n",
    "  5m    5m    1 kubelet, minikube spec.containers{helloworld} Normal    Created     Created container\n",
    "  5m    5m    1 kubelet, minikube spec.containers{helloworld} Normal    Started     Started container\n",
    "  4m    1m    20  kubelet, minikube spec.containers{helloworld} Warning   Unhealthy   Readiness probe failed: Get http://172.17.0.4:90/: dial tcp 172.17.0.4:90: getsockopt: connection refused\n",
    "MacbookHome:04_02 Application health checks karthik$\n",
    "```\n",
    "\n",
    "### Simulate a failing deployment that fails a liveness probe\n",
    "\n",
    "Next, we will simulate a bad helloworld pod that fails a liveness probe. Instead of checking port 80 like the last example, we will run a liveness check on port 90 to simulate a failing scenario. Thus, our yaml now is:\n",
    "\n",
    "```\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: helloworld-deployment-with-bad-liveness-probe\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: helloworld\n",
    "  replicas: 1 # tells deployment to run 1 pods matching the template\n",
    "  template: # create pods using pod definition in this template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: helloworld\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: helloworld\n",
    "        image: karthequian/helloworld:latest\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "        livenessProbe:\n",
    "          # length of time to wait for a pod to initialize\n",
    "          # after pod startup, before applying health checking\n",
    "          initialDelaySeconds: 10\n",
    "          # How often (in seconds) to perform the probe.\n",
    "          periodSeconds: 5\n",
    "          # Amount of time to wait before timing out\n",
    "          timeoutSeconds: 1\n",
    "          # Kubernetes will try failureThreshold times before giving up and restarting the Pod\n",
    "          failureThreshold: 2\n",
    "          # Probe for http\n",
    "          httpGet:\n",
    "            # Path to probe\n",
    "            path: /\n",
    "            # Port to probe\n",
    "            port: 90\n",
    "```\n",
    "\n",
    "\n",
    "We will run this yaml with the command `kubectl create -f helloworld-with-bad-liveness-probe.yaml`. After about a minute, we will notice that our pod is still not in a read state when we run the `kubectl get pods` command.\n",
    "\n",
    "```\n",
    "MacbookHome:04_02 Application health checks karthik$ kubectl create -f helloworld-with-bad-liveness-probe.yaml\n",
    "deployment \"helloworld-deployment-with-bad-liveness-probe\" created\n",
    "MacbookHome:04_02 Application health checks karthik$ kubectl get deployments\n",
    "NAME                                            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\n",
    "helloworld-deployment-with-bad-liveness-probe   1         1         1            1           5s\n",
    "MacbookHome:04_02 Application health checks karthik$ kubectl get pods\n",
    "NAME                                                              READY     STATUS        RESTARTS   AGE\n",
    "helloworld-deployment-with-bad-liveness-probe-77759fbbbf-kljs4    1/1       Running       0          9s\n",
    "MacbookHome:04_02 Application health checks karthik$ kubectl get pods\n",
    "NAME                                                              READY     STATUS        RESTARTS   AGE\n",
    "helloworld-deployment-with-bad-liveness-probe-77759fbbbf-kljs4    1/1       Running       1          26s\n",
    "MacbookHome:04_02 Application health checks karthik$ kubectl get pods\n",
    "NAME                                                             READY     STATUS    RESTARTS   AGE\n",
    "helloworld-deployment-with-bad-liveness-probe-77759fbbbf-kljs4   1/1       Running   1          32s\n",
    "MacbookHome:04_02 Application health checks karthik$ kubectl get pods\n",
    "NAME                                                             READY     STATUS    RESTARTS   AGE\n",
    "helloworld-deployment-with-bad-liveness-probe-77759fbbbf-kljs4   1/1       Running   2          38s\n",
    "MacbookHome:04_02 Application health checks karthik$ kubectl get pods\n",
    "NAME                                                             READY     STATUS    RESTARTS   AGE\n",
    "helloworld-deployment-with-bad-liveness-probe-77759fbbbf-kljs4   1/1       Running   3          50s\n",
    "MacbookHome:04_02 Application health checks karthik$ kubectl get pods\n",
    "NAME                                                             READY     STATUS             RESTARTS   AGE\n",
    "helloworld-deployment-with-bad-liveness-probe-77759fbbbf-kljs4   0/1       CrashLoopBackOff   3          1m\n",
    "```\n",
    "\n",
    "When we describe our pod, we notice that it is failing the liveness probe:\n",
    "\n",
    "```\n",
    "MacbookHome:04_02 Application health checks karthik$ kubectl describe po helloworld-deployment-with-bad-liveness-probe-77759fbbbf-kljs4\n",
    "Name:   helloworld-deployment-with-bad-liveness-probe-77759fbbbf-kljs4\n",
    "Namespace:  default\n",
    "Node:   minikube/192.168.99.101\n",
    "Start Time: Sun, 29 Oct 2017 01:47:59 -0500\n",
    "Labels:   app=helloworld\n",
    "    pod-template-hash=3331596669\n",
    "Annotations:  kubernetes.io/created-by={\"kind\":\"SerializedReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"ReplicaSet\",\"namespace\":\"default\",\"name\":\"helloworld-deployment-with-bad-liveness-probe-77759fbbbf\",\"uid\":...\n",
    "Status:   Running\n",
    "IP:   172.17.0.4\n",
    "Controllers:  ReplicaSet/helloworld-deployment-with-bad-liveness-probe-77759fbbbf\n",
    "Containers:\n",
    "  helloworld:\n",
    "    Container ID: docker://41cfe81a3327b03831440e861eb2164b8a9f601ce630f024c76412d903781922\n",
    "    Image:    karthequian/helloworld:latest\n",
    "    Image ID:   docker-pullable://karthequian/helloworld@sha256:165f87263f1775f0bf91022b48a51265357ba9f36bc3882f5ecdefc7f8ef8f6d\n",
    "    Port:   80/TCP\n",
    "    State:    Running\n",
    "      Started:    Sun, 29 Oct 2017 01:48:47 -0500\n",
    "    Last State:   Terminated\n",
    "      Reason:   Completed\n",
    "      Exit Code:  0\n",
    "      Started:    Sun, 29 Oct 2017 01:48:31 -0500\n",
    "      Finished:   Sun, 29 Oct 2017 01:48:46 -0500\n",
    "    Ready:    True\n",
    "    Restart Count:  3\n",
    "    Liveness:   http-get http://:90/ delay=10s timeout=1s period=5s #success=1 #failure=2\n",
    "    Environment:  <none>\n",
    "    Mounts:\n",
    "      /var/run/secrets/kubernetes.io/serviceaccount from default-token-x8qf6 (ro)\n",
    "Conditions:\n",
    "  Type    Status\n",
    "  Initialized   True\n",
    "  Ready   True\n",
    "  PodScheduled  True\n",
    "Volumes:\n",
    "  default-token-x8qf6:\n",
    "    Type: Secret (a volume populated by a Secret)\n",
    "    SecretName: default-token-x8qf6\n",
    "    Optional: false\n",
    "QoS Class:  BestEffort\n",
    "Node-Selectors: <none>\n",
    "Tolerations:  <none>\n",
    "Events:\n",
    "  FirstSeen LastSeen  Count From      SubObjectPath     Type    Reason      Message\n",
    "  --------- --------  ----- ----      -------------     --------  ------      -------\n",
    "  58s   58s   1 default-scheduler         Normal    Scheduled   Successfully assigned helloworld-deployment-with-bad-liveness-probe-77759fbbbf-kljs4 to minikube\n",
    "  58s   58s   1 kubelet, minikube         Normal    SuccessfulMountVolume MountVolume.SetUp succeeded for volume \"default-token-x8qf6\"\n",
    "  47s   12s   4 kubelet, minikube spec.containers{helloworld} Warning   Unhealthy   Liveness probe failed: Get http://172.17.0.4:90/: dial tcp 172.17.0.4:90: getsockopt: connection refused\n",
    "  58s   11s   4 kubelet, minikube spec.containers{helloworld} Normal    Pulling     pulling image \"karthequian/helloworld:latest\"\n",
    "  57s   11s   4 kubelet, minikube spec.containers{helloworld} Normal    Pulled      Successfully pulled image \"karthequian/helloworld:latest\"\n",
    "  57s   11s   4 kubelet, minikube spec.containers{helloworld} Normal    Created     Created container\n",
    "  41s   11s   3 kubelet, minikube spec.containers{helloworld} Normal    Killing     Killing container with id docker://helloworld:Container failed liveness probe.. Container will be killed and recreated.\n",
    "  57s   10s   4 kubelet, minikube spec.containers{helloworld} Normal    Started     Started container\n",
    "MacbookHome:04_02 Application health checks karthik$\n",
    "```\n",
    "\n",
    "And finally, when we check the deployment, we notice that deployment is not available as well.\n",
    "```\n",
    "MacbookHome:04_02 Application health checks karthik$ kubectl get deployments\n",
    "NAME                                            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\n",
    "helloworld-deployment-with-bad-liveness-probe   1         1         1            0           3m\n",
    "```\n",
    "\n",
    "To summarize, we've learned that we can use readiness and liveness probes to check the status of our pods and use them to restart pods when necessary and check pod health.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can modify the image being used in place in the deployment using `kubectl set image` and monitor the upgrade using `kubectl rollout history` and `--record=true` on `kubectl create`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling application upgrades\n",
    "\n",
    "## Chapter Goals\n",
    "1. Upgrade a deployment from 1 version to another\n",
    "2. Rollback the deployment to the 1st version\n",
    "\n",
    "### Upgrade a deployment from 1 version to another\n",
    "\n",
    "Let's deploy our initial version of our application `kubectl create -f helloworld-black.yaml --record`. After the deployment and service has completed, let's expose this via a nodeport by `minikube service navbar-service`. This will bring up the helloworld UI that we have seen before. We added the `--record` to this because we want to record our rollout history that I'll talk about later on.\n",
    "\n",
    "Looking at the deployment, we see that there are 3 desired, current and ready replicas.\n",
    "\n",
    "As developers, we're required to make changes to our applications and get these deployed. The rollout functionality of Kubernetes assists with upgrades because it allows us to upgrade the code without any downtime. In our application, I'd like to update the nav bar to a blue color rather than what it is right now. I'll package this in a container with a new label called \"blue\".\n",
    "\n",
    "To update the image, I run this command: `kubectl set image deployment/navbar-deployment helloworld=karthequian/helloworld:blue`. This sets the image from what it is currently (karthequian/helloworld:black) to karthequian/helloworld:blue.\n",
    "\n",
    "The deployment will update, and if you look at the webpage, you'll see the updated text.\n",
    "\n",
    "Let's take a look at what happened here. When the deployment was edited, a new result set was created for it. Running the `kubectl get rs` command shows us this. One result set with 3 desired, current and ready pods, and another with 0.\n",
    "\n",
    "We can also take a look at the rollout history by typing `kubectl rollout history deployment/navbar-deployment`.\n",
    "\n",
    "### Rollback the deployment to the 1st version\n",
    "To rollback the deployment, we use the rollout undo command `kubectl rollout undo deployment/navbar-deployment`. This will revert our changes back to the previous version.\n",
    "\n",
    "Our webpage will be back to the Lionel version of the deployment.\n",
    "\n",
    "In a real world setting, you might have a longer history, and might want to rollback to a specific version. To do this, add a `--to-revision=version` to the specific version you want to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file helloworld-black.yaml\n",
    "\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: navbar-deployment\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: helloworld\n",
    "  replicas: 3 # tells deployment to run 3 pods matching the template\n",
    "  template: # create pods using pod definition in this template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: helloworld\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: helloworld\n",
    "        image: karthequian/helloworld:black\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: navbar-service\n",
    "spec:\n",
    "  # if your cluster supports it, uncomment the following to automatically create\n",
    "  # an external load-balanced IP for the frontend service.\n",
    "  type: NodePort\n",
    "  ports:\n",
    "  - port: 80\n",
    "    protocol: TCP\n",
    "    targetPort: 80\n",
    "  selector:\n",
    "    app: helloworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic troubleshooting techniques\n",
    "\n",
    "## Chapter Goals\n",
    "1. Kubernetes Techniques\n",
    "2. Looking at Log files\n",
    "3. Executing commands in a container\n",
    "\n",
    "### Overview\n",
    "Sometimes things don't work as they should in your deployments, and you'd like to take a closer look to debug issues or understand what's going on. There are 3 techniques I use from a day to day basis when I work with kubernetes. Let me show you what these are.\n",
    "\n",
    "### Kubernetes Techniques\n",
    "When things are not deploying as expected, or things seem to be taking a while, I describe the deployments and pods associated with the deployments to look for errors.\n",
    "\n",
    "Let's run the helloworld application that is bundled with this section by typing `kubectl create -f helloworld-with-bad-pod.yaml`.\n",
    "\n",
    "As it's starting up, we can run a `kubectl get deployments` and a `kubectl describe deployment bad-helloworld-deployment`.\n",
    "\n",
    "We notice that we have 0 available pods in the deployment that signals that there is something going on with the pod.\n",
    "\n",
    "If we introspect pods with a `kubectl get pods`, we see that the `bad-helloworld-deployment` pod is in an image pull backoff state and isn't ready.\n",
    "\n",
    "Describing the pod with `kubectl describe pod bad-helloworld-deployment-7bb4b7466-f6nkm`, will show me that kubernetes is having trouble pull the pod from the repository, either because it doesn't exist, or because we're missing the repository credentials.\n",
    "\n",
    "### Looking at log files\n",
    "Another technique I end up using a lot to track pod progress is looking at the log files for a pod. If you write your logs to standard out, you can get to them by the command `kubectl logs <pod_name>`. This will return the log statements that are being written by your application in the pod.\n",
    "\n",
    "### Executing commands in a container\n",
    "Finally, sometimes it is necessary to exec into the actual container running the pod to look for errors, or state. To do this, run the exec command `kubectl exec -it <pod-name> -c <container-name> /bin/bash` where -it is an interactive terminal and -c is the flag to specify the container name. Finally we want a bash style terminal.\n",
    "\n",
    "This drops us into the container, and we can introspect into the details of our application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file service.yaml\n",
    "\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: helloworld-service\n",
    "spec:\n",
    "  # if your cluster supports it, uncomment the following to automatically create\n",
    "  # an external load-balanced IP for the frontend service.\n",
    "  type: LoadBalancer\n",
    "  ports:\n",
    "  - port: 80\n",
    "    protocol: TCP\n",
    "    targetPort: 80\n",
    "  selector:\n",
    "    app: helloworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file helloworld-deployment.yaml\n",
    "\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: helloworld-deployment\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: helloworld\n",
    "  replicas: 1 # tells deployment to run 1 pods matching the template\n",
    "  template: # create pods using pod definition in this template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: helloworld\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: helloworld\n",
    "        image: karthequian/helloworld:latest\n",
    "        ports:\n",
    "        - containerPort: 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file helloworld-with-bad-pod.yaml\n",
    "\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: bad-helloworld-deployment\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: bad-helloworld\n",
    "  replicas: 1 # tells deployment to run 1 pods matching the template\n",
    "  template: # create pods using pod definition in this template\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: bad-helloworld\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: helloworld\n",
    "        image: karthequian/unkown-pod:latest\n",
    "        ports:\n",
    "        - containerPort: 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe deployment helloworld-deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl describe pod/helloworld-deployment-2fsf312"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps -ef # list all of the processes in the pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a more real world example\n",
    "\n",
    "In this chapter, we'll take the popular Kubernetes guestbook, and attempt to run it! You can read more about the guestbook here: [https://kubernetes.io/docs/tutorials/stateless-application/guestbook/]\n",
    "\n",
    "Run the guestbook by executing `kubectl create -f guestbook.yaml`\n",
    "\n",
    "You'll see something like this:\n",
    "\n",
    "```\n",
    "$ kubectl create -f guestbook.yaml\n",
    "deployment.apps/redis-master created\n",
    "service/redis-master created\n",
    "deployment.apps/redis-slave created\n",
    "service/redis-slave created\n",
    "deployment.apps/frontend created\n",
    "service/frontend created\n",
    "```\n",
    "\n",
    "Load up the guestbook by running the command `minikube service frontend`. The output would look like this:\n",
    "```\n",
    "$ minikube service frontend\n",
    "|-----------|----------|-------------|---------------------------|\n",
    "| NAMESPACE |   NAME   | TARGET PORT |            URL            |\n",
    "|-----------|----------|-------------|---------------------------|\n",
    "| default   | frontend |          80 | http://192.168.64.2:31824 |\n",
    "|-----------|----------|-------------|---------------------------|\n",
    "🎉  Opening service default/frontend in default browser...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file guestbook.yaml\n",
    "\n",
    "apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: redis-master\n",
    "  labels:\n",
    "    app: redis\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: redis\n",
    "      role: master\n",
    "      tier: backend\n",
    "  replicas: 1\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: redis\n",
    "        role: master\n",
    "        tier: backend\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: master\n",
    "        image: k8s.gcr.io/redis:e2e  # or just image: redis\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: 100m\n",
    "            memory: 100Mi\n",
    "        ports:\n",
    "        - containerPort: 6379\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: redis-master\n",
    "  labels:\n",
    "    app: redis\n",
    "    role: master\n",
    "    tier: backend\n",
    "spec:\n",
    "  ports:\n",
    "  - port: 6379\n",
    "    targetPort: 6379\n",
    "  selector:\n",
    "    app: redis\n",
    "    role: master\n",
    "    tier: backend\n",
    "---\n",
    "apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: redis-slave\n",
    "  labels:\n",
    "    app: redis\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: redis\n",
    "      role: slave\n",
    "      tier: backend\n",
    "  replicas: 2\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: redis\n",
    "        role: slave\n",
    "        tier: backend\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: slave\n",
    "        image: gcr.io/google_samples/gb-redisslave:v3\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: 100m\n",
    "            memory: 100Mi\n",
    "        env:\n",
    "        - name: GET_HOSTS_FROM\n",
    "          value: dns\n",
    "          # Using `GET_HOSTS_FROM=dns` requires your cluster to\n",
    "          # provide a dns service. As of Kubernetes 1.3, DNS is a built-in\n",
    "          # service launched automatically. However, if the cluster you are using\n",
    "          # does not have a built-in DNS service, you can instead\n",
    "          # access an environment variable to find the master\n",
    "          # service's host. To do so, comment out the 'value: dns' line above, and\n",
    "          # uncomment the line below:\n",
    "          # value: env\n",
    "        ports:\n",
    "        - containerPort: 6379\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: redis-slave\n",
    "  labels:\n",
    "    app: redis\n",
    "    role: slave\n",
    "    tier: backend\n",
    "spec:\n",
    "  ports:\n",
    "  - port: 6379\n",
    "  selector:\n",
    "    app: redis\n",
    "    role: slave\n",
    "    tier: backend\n",
    "---\n",
    "apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: frontend\n",
    "  labels:\n",
    "    app: guestbook\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: guestbook\n",
    "      tier: frontend\n",
    "  replicas: 3\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: guestbook\n",
    "        tier: frontend\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: php-redis\n",
    "        image: gcr.io/google-samples/gb-frontend:v4\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: 100m\n",
    "            memory: 100Mi\n",
    "        env:\n",
    "        - name: GET_HOSTS_FROM\n",
    "          value: dns\n",
    "          # Using `GET_HOSTS_FROM=dns` requires your cluster to\n",
    "          # provide a dns service. As of Kubernetes 1.3, DNS is a built-in\n",
    "          # service launched automatically. However, if the cluster you are using\n",
    "          # does not have a built-in DNS service, you can instead\n",
    "          # access an environment variable to find the master\n",
    "          # service's host. To do so, comment out the 'value: dns' line above, and\n",
    "          # uncomment the line below:\n",
    "          # value: env\n",
    "        ports:\n",
    "        - containerPort: 80\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: frontend\n",
    "  labels:\n",
    "    app: guestbook\n",
    "    tier: frontend\n",
    "spec:\n",
    "  # comment or delete the following line if you want to use a LoadBalancer\n",
    "  type: NodePort \n",
    "  # if your cluster supports it, uncomment the following to automatically create\n",
    "  # an external load-balanced IP for the frontend service.\n",
    "  # type: LoadBalancer\n",
    "  ports:\n",
    "  - port: 80\n",
    "  selector:\n",
    "    app: guestbook\n",
    "    tier: frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create -f guestbook.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with configmaps\n",
    "\n",
    "## Chapter Goals\n",
    "1. Learn how to declare a configmap\n",
    "2. Understand how to call a configmap from a deployment\n",
    "\n",
    "### Learn how to declare a configmap\n",
    "Applications require a way for us to pass data to them that can be changed at deploy time. Examples of this might be log-levels or urls of external systems that the application might need at startup time. Instead of hardcoding these values, we can use a configmap in kubernetes, and pass these values as environment variables to the container.\n",
    "\n",
    "We will take an example of \"log_level\", and pass the value \"debug\" to a pod via a configmap in this example.\n",
    "\n",
    "To create a configmap for this literal type `kubectl create configmap logger --from-literal=log_level=debug`\n",
    "\n",
    "To see all your configmaps: `kubectl get configmaps`\n",
    "\n",
    "To read the value in the logger configmap: `kubectl get configmap/logger -o yaml`\n",
    "\n",
    "To edit the value, we can run `kubectl edit configmap/logger`\n",
    "\n",
    "### Understand how to call a configmap from a deployment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: logreader\n",
    "  labels:\n",
    "    app: logreader\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: logreader\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: logreader\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: logreader\n",
    "        image: karthequian/reader:latest\n",
    "        env:\n",
    "        - name: log_level\n",
    "          value: \"error\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create configmap logger --from-literal=log_level=debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: logreader-dynamic\n",
    "  labels:\n",
    "    app: logreader-dynamic\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: logreader-dynamic\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: logreader-dynamic\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: logreader\n",
    "        image: karthequian/reader:latest\n",
    "        env:\n",
    "        - name: log_level\n",
    "          valueFrom:\n",
    "            configMapKeyRef:\n",
    "              name: logger #Read from a configmap called log-level\n",
    "              key: log_level  #Read the key called log_level\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get configmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get configmap/logger -o yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with secrets\n",
    "\n",
    "## Chapter Goals\n",
    "1. Learn how to declare a secret\n",
    "2. Understand how to add a secret to a deployment\n",
    "\n",
    "### Learn how to declare a secret\n",
    "Just like configuration data, applications might also require other data that might be of more sensitive in nature- for example database passwords, or API tokens. Passing these in the yaml for a deployment or pod would make them visible to everyone.\n",
    "\n",
    "In these usecases, use a secret to encapsulate sensitive data.\n",
    "\n",
    "To create a secret: `kubectl create secret generic apikey --from-literal=api_key=123456789`\n",
    "\n",
    "Notice that we can't read the value of the secret directly:\n",
    "`kubectl get secret apikey -o yaml`\n",
    "\n",
    "### Understand how to add a secret to a deployment\n",
    "\n",
    "Adding a secret to a deployment is similar to what we did for configmaps. You can add a secret to the env portion, and start up the deployment with:\n",
    "`kubectl create -f secretreader-deployment.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create secret generic apikey --from-literal=api_key=1124124 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get secret apikey -o yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: secretreader\n",
    "  labels:\n",
    "    name: secretreader\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels: \n",
    "      name: secretreader\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        name: secretreader\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: secretreader\n",
    "        image: karthequian/secretreader:latest\n",
    "        env:\n",
    "        - name: api_key\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: apikey\n",
    "              key: api_key\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobs in Kubernetes\n",
    "\n",
    "## Chapter Goals\n",
    "1. How to run jobs\n",
    "2. How to run cron jobs\n",
    "\n",
    "### How to run jobs\n",
    "Jobs are a construct that run a pod once, and then stop. However, unlike pods in deployments, the output of the job is kept around until you decide to remove it.\n",
    "\n",
    "Running a job is similar to running a deployment, and we can create this by `kubectl create -f simplejob.yaml`\n",
    "\n",
    "To see the output of the job: `kubectl get jobs`\n",
    "\n",
    "You can find the pod that ran by doing a `kubectl get pods`, and then get the logs from it as well.\n",
    "\n",
    "### How to run cron jobs\n",
    "Cron jobs are like jobs, but they run periodically.\n",
    "\n",
    "Start your cron by running `kubectl create -f cronjob.yaml`\n",
    "\n",
    "We can use the cronjob api to view your cronjobs: `kubectl get cronjobs`. It adds the last schedule date\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "apiVersion: batch/v1beta1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: hellocron\n",
    "spec:\n",
    "  schedule: \"*/1 * * * *\" #Runs every minute (cron syntax) or @hourly.\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: hellocron\n",
    "            image: busybox\n",
    "            args:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "            - date; echo Hello from your Kubernetes cluster\n",
    "          restartPolicy: OnFailure #could also be Always or Never\n",
    "  suspend: false #Set to true if you want to suspend in the future\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: finalcountdown\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      name: finalcountdown\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: counter\n",
    "        image: busybox\n",
    "        command:\n",
    "         - bin/sh\n",
    "         - -c\n",
    "         - \"for i in 9 8 7 6 5 4 3 2 1 ; do echo $i ; done\"\n",
    "      restartPolicy: Never #could also be Always or OnFailure\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daemonsets and Statefulsets\n",
    "\n",
    "## Chapter Goals\n",
    "1. How to run daemonsets\n",
    "2. How to run statefulsets\n",
    "\n",
    "## Daemonsets\n",
    "\n",
    "Daemonsets: A DaemonSet ensures that all Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. Examples of a daemon set would be running your logging or monitoring agent on your nodes.\n",
    "\n",
    "First, you'll want to make sure you tag minikube with a label of `kubectl label node/minikube infra=development` to label the node first.\n",
    "\n",
    "In the example, I will just run a simple busybox image as a daemonset, and then run daemonset examples to show how you can tag things to run on specific nodes\n",
    "`kubectl create -f daemonset.yaml` will run on the nodes\n",
    "\n",
    "`kubectl create -f daemonset-infra-development.yaml`  will only run on nodes labeled `infra=development` (as shown above in the label)\n",
    "\n",
    "`kubectl create -f daemonset-infra-prod.yaml`  will only run on nodes labeled `infra=production`\n",
    "\n",
    "\n",
    "## Statefulsets\n",
    "\n",
    "Statefulsets Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of their Pods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: DaemonSet\n",
    "metadata:\n",
    "  name: example-daemonset\n",
    "  namespace: default\n",
    "  labels:\n",
    "    k8s-app: example-daemonset\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      name: example-daemonset\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        name: example-daemonset\n",
    "    spec:\n",
    "      #nodeSelector: minikube # Specify if you want to run on specific nodes\n",
    "      containers:\n",
    "      - name: example-daemonset\n",
    "        image: busybox\n",
    "        args:\n",
    "        - /bin/sh\n",
    "        - -c\n",
    "        - date; sleep 1000\n",
    "        resources:\n",
    "          limits:\n",
    "            memory: 200Mi\n",
    "          requests:\n",
    "            cpu: 100m\n",
    "            memory: 200Mi\n",
    "      terminationGracePeriodSeconds: 30\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: zk-hs\n",
    "  labels:\n",
    "    app: zk\n",
    "spec:\n",
    "  ports:\n",
    "  - port: 2888\n",
    "    name: server\n",
    "  - port: 3888\n",
    "    name: leader-election\n",
    "  clusterIP: None\n",
    "  selector:\n",
    "    app: zk\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: zk-cs\n",
    "  labels:\n",
    "    app: zk\n",
    "spec:\n",
    "  ports:\n",
    "  - port: 2181\n",
    "    name: client\n",
    "  selector:\n",
    "    app: zk\n",
    "---\n",
    "apiVersion: policy/v1beta1\n",
    "kind: PodDisruptionBudget\n",
    "metadata:\n",
    "  name: zk-pdb\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: zk\n",
    "  maxUnavailable: 1\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: StatefulSet\n",
    "metadata:\n",
    "  name: zk\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: zk\n",
    "  serviceName: zk-hs\n",
    "  replicas: 3\n",
    "  updateStrategy:\n",
    "    type: RollingUpdate\n",
    "  podManagementPolicy: OrderedReady\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: zk\n",
    "    spec:\n",
    "      affinity:\n",
    "        podAntiAffinity:\n",
    "          requiredDuringSchedulingIgnoredDuringExecution:\n",
    "            - labelSelector:\n",
    "                matchExpressions:\n",
    "                  - key: \"app\"\n",
    "                    operator: In\n",
    "                    values:\n",
    "                    - zk\n",
    "              topologyKey: \"kubernetes.io/hostname\"\n",
    "      containers:\n",
    "      - name: kubernetes-zookeeper\n",
    "        imagePullPolicy: Always\n",
    "        image: \"k8s.gcr.io/kubernetes-zookeeper:1.0-3.4.10\"\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"0.5\"\n",
    "        ports:\n",
    "        - containerPort: 2181\n",
    "          name: client\n",
    "        - containerPort: 2888\n",
    "          name: server\n",
    "        - containerPort: 3888\n",
    "          name: leader-election\n",
    "        command:\n",
    "        - sh\n",
    "        - -c\n",
    "        - \"start-zookeeper \\\n",
    "          --servers=3 \\\n",
    "          --data_dir=/var/lib/zookeeper/data \\\n",
    "          --data_log_dir=/var/lib/zookeeper/data/log \\\n",
    "          --conf_dir=/opt/zookeeper/conf \\\n",
    "          --client_port=2181 \\\n",
    "          --election_port=3888 \\\n",
    "          --server_port=2888 \\\n",
    "          --tick_time=2000 \\\n",
    "          --init_limit=10 \\\n",
    "          --sync_limit=5 \\\n",
    "          --heap=512M \\\n",
    "          --max_client_cnxns=60 \\\n",
    "          --snap_retain_count=3 \\\n",
    "          --purge_interval=12 \\\n",
    "          --max_session_timeout=40000 \\\n",
    "          --min_session_timeout=4000 \\\n",
    "          --log_level=INFO\"\n",
    "        readinessProbe:\n",
    "          exec:\n",
    "            command:\n",
    "            - sh\n",
    "            - -c\n",
    "            - \"zookeeper-ready 2181\"\n",
    "          initialDelaySeconds: 10\n",
    "          timeoutSeconds: 5\n",
    "        livenessProbe:\n",
    "          exec:\n",
    "            command:\n",
    "            - sh\n",
    "            - -c\n",
    "            - \"zookeeper-ready 2181\"\n",
    "          initialDelaySeconds: 10\n",
    "          timeoutSeconds: 5\n",
    "        volumeMounts:\n",
    "        - name: datadir\n",
    "          mountPath: /var/lib/zookeeper\n",
    "      securityContext:\n",
    "        runAsUser: 1000\n",
    "        fsGroup: 1000\n",
    "  volumeClaimTemplates:\n",
    "  - metadata:\n",
    "      name: datadir\n",
    "    spec:\n",
    "      accessModes: [ \"ReadWriteOnce\" ]\n",
    "      resources:\n",
    "        requests:\n",
    "          storage: 10Gi\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
